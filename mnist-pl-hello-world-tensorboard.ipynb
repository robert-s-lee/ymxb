{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "01-mnist-hello-world.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3710jvsc74a57bd016ccfe9cb28ae5249e710061f0e16cc760a03460c0b5a0514c93c438190c8559",
      "display_name": "Python 3.7.10 64-bit ('gridai': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robert-s-lee/ymxb/blob/master/mnist-pl-hello-world.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7XbLCXGkll9"
      },
      "source": [
        "# Introduction to Pytorch Lightning ‚ö°\n",
        "\n",
        "In this notebook, we'll go over the basics of lightning by preparing models to train on the [MNIST Handwritten Digits dataset](https://en.wikipedia.org/wiki/MNIST_database).\n",
        "\n",
        "---\n",
        "  - Give us a ‚≠ê [on Github](https://www.github.com/PytorchLightning/pytorch-lightning/)\n",
        "  - Check out [the documentation](https://pytorch-lightning.readthedocs.io/en/latest/)\n",
        "  - Join us [on Slack](https://join.slack.com/t/pytorch-lightning/shared_invite/zt-pw5v393p-qRaDgEk24~EjiZNBpSQFgQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LODD6w9ixlT"
      },
      "source": [
        "### Setup  \n",
        "Lightning is easy to install. Simply ```pip install pytorch-lightning```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK7-Gg69kMnG"
      },
      "source": [
        "! pip install pytorch-lightning"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /opt/miniconda3/lib/python3.9/site-packages (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (1.20.3)\n",
            "Requirement already satisfied: pyDeprecate==0.3.0 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (0.3.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: PyYAML<=5.4.1,>=5.1 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (5.4.1)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (2.4.1)\n",
            "Requirement already satisfied: torchmetrics>=0.2.0 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (0.3.2)\n",
            "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (20.9)\n",
            "Requirement already satisfied: fsspec[http]>=2021.4.0 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (2021.5.0)\n",
            "Requirement already satisfied: torch>=1.4 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (4.54.1)\n",
            "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.9/site-packages (from fsspec[http]>=2021.4.0->pytorch-lightning) (2.25.0)\n",
            "Requirement already satisfied: aiohttp in /opt/miniconda3/lib/python3.9/site-packages (from fsspec[http]>=2021.4.0->pytorch-lightning) (3.7.4.post0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/miniconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (1.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (21.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/miniconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (3.10.0.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (5.1.0)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /opt/miniconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/miniconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (3.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /opt/miniconda3/lib/python3.9/site-packages (from packaging->pytorch-lightning) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/miniconda3/lib/python3.9/site-packages (from requests->fsspec[http]>=2021.4.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /opt/miniconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda3/lib/python3.9/site-packages (from requests->fsspec[http]>=2021.4.0->pytorch-lightning) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.9/site-packages (from requests->fsspec[http]>=2021.4.0->pytorch-lightning) (2020.12.5)\n",
            "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.9/site-packages (from fsspec[http]>=2021.4.0->pytorch-lightning) (2.25.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.38.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (2.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.30.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.17.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.36.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (1.20.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (51.0.0.post20201207)\n",
            "Requirement already satisfied: six>=1.10.0 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/miniconda3/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/miniconda3/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (51.0.0.post20201207)\n",
            "Requirement already satisfied: six>=1.10.0 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/miniconda3/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/miniconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.30.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /opt/miniconda3/lib/python3.9/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/miniconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/miniconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.9/site-packages (from fsspec[http]>=2021.4.0->pytorch-lightning) (2.25.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/miniconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/miniconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (3.10.0.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (1.20.3)\n",
            "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (20.9)\n",
            "Requirement already satisfied: torch>=1.4 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/miniconda3/lib/python3.9/site-packages (from requests->fsspec[http]>=2021.4.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (5.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4_TYnt_keJi"
      },
      "source": [
        "import os\n",
        "from argparse import ArgumentParser\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics.functional import accuracy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "parser = ArgumentParser()\n",
        "parser.add_argument('--epochs', default=3, type=int)\n",
        "parser.add_argument('--gpu', default=0, type=int)\n",
        "parser.add_argument('--batch_size', default=32, type=int)\n",
        "parser.add_argument('--hidden_size', default=64, type=int)\n",
        "parser.add_argument('--lr', default=2e-4, type=float)\n",
        "parser.add_argument('--data', default=os.getcwd(), type=str)\n",
        "\n",
        "if __name__ == \"__main__.py\":    \n",
        "    args = parser.parse_args()      \n",
        "else:\n",
        "    args = parser.parse_args(\"\")    # take defaults in Jupyter \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNpOoBeIjscS"
      },
      "source": [
        "## A more complete MNIST Lightning Module Example\n",
        "\n",
        "That wasn't so hard was it?\n",
        "\n",
        "Now that we've got our feet wet, let's dive in a bit deeper and write a more complete `LightningModule` for MNIST...\n",
        "\n",
        "This time, we'll bake in all the dataset specific pieces directly in the `LightningModule`. This way, we can avoid writing extra code at the beginning of our script every time we want to run it.\n",
        "\n",
        "---\n",
        "\n",
        "### Note what the following built-in functions are doing:\n",
        "\n",
        "1. [prepare_data()](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.prepare_data) üíæ\n",
        "    - This is where we can download the dataset. We point to our desired dataset and ask torchvision's `MNIST` dataset class to download if the dataset isn't found there.\n",
        "    - **Note we do not make any state assignments in this function** (i.e. `self.something = ...`)\n",
        "\n",
        "2. [setup(stage)](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning-module.html#setup) ‚öôÔ∏è\n",
        "    - Loads in data from file and prepares PyTorch tensor datasets for each split (train, val, test). \n",
        "    - Setup expects a 'stage' arg which is used to separate logic for 'fit' and 'test'.\n",
        "    - If you don't mind loading all your datasets at once, you can set up a condition to allow for both 'fit' related setup and 'test' related setup to run whenever `None` is passed to `stage` (or ignore it altogether and exclude any conditionals).\n",
        "    - **Note this runs across all GPUs and it *is* safe to make state assignments here**\n",
        "\n",
        "3. [x_dataloader()](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning-module.html#data-hooks) ‚ôªÔ∏è\n",
        "    - `train_dataloader()`, `val_dataloader()`, and `test_dataloader()` all return PyTorch `DataLoader` instances that are created by wrapping their respective datasets that we prepared in `setup()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DNItffri95Q"
      },
      "source": [
        "class LitMNIST(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, data_dir=args.data, hidden_size=args.hidden_size, learning_rate=args.lr):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Set our init args as class attributes\n",
        "        self.data_dir = data_dir\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Hardcode some dataset specific attributes\n",
        "        self.num_classes = 10\n",
        "        self.dims = (1, 28, 28)\n",
        "        channels, width, height = self.dims\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "\n",
        "        # Define PyTorch model\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(channels * width * height, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size, self.num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.nll_loss(logits, y)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.nll_loss(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = accuracy(preds, y)\n",
        "\n",
        "        # Calling self.log will surface up scalars for you in TensorBoard\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_acc', acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # Here we just reuse the validation_step for testing\n",
        "        return self.validation_step(batch, batch_idx)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    ####################\n",
        "    # DATA RELATED HOOKS\n",
        "    ####################\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # download\n",
        "        MNIST(self.data_dir, train=True, download=True)\n",
        "        MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == 'fit' or stage is None:\n",
        "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
        "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == 'test' or stage is None:\n",
        "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.mnist_train, batch_size=args.batch_size)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.mnist_val, batch_size=args.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.mnist_test, batch_size=args.batch_size)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb0U5Rk2kLBy"
      },
      "source": [
        "model = LitMNIST()\n",
        "trainer = pl.Trainer(gpus=args.gpu, max_epochs=args.epochs, progress_bar_refresh_rate=20)\n",
        "trainer.fit(model)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\n",
            "  | Name  | Type       | Params\n",
            "-------------------------------------\n",
            "0 | model | Sequential | 55.1 K\n",
            "-------------------------------------\n",
            "55.1 K    Trainable params\n",
            "0         Non-trainable params\n",
            "55.1 K    Total params\n",
            "0.220     Total estimated model params size (MB)\n",
            "Epoch 0:   0%|          | 0/1876 [00:00<?, ?it/s] /opt/miniconda3/envs/gridai/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/opt/miniconda3/envs/gridai/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Epoch 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1720/1876 [00:10<00:00, 164.02it/s, loss=0.343, v_num=1, val_loss=2.330, val_acc=0.0781]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1740/1876 [00:10<00:00, 164.23it/s, loss=0.343, v_num=1, val_loss=2.330, val_acc=0.0781]\n",
            "Epoch 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1780/1876 [00:10<00:00, 165.15it/s, loss=0.343, v_num=1, val_loss=2.330, val_acc=0.0781]\n",
            "Epoch 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1820/1876 [00:10<00:00, 166.02it/s, loss=0.343, v_num=1, val_loss=2.330, val_acc=0.0781]\n",
            "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1876/1876 [00:11<00:00, 166.91it/s, loss=0.256, v_num=1, val_loss=0.261, val_acc=0.924] \n",
            "Epoch 1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1720/1876 [00:10<00:00, 170.72it/s, loss=0.245, v_num=1, val_loss=0.261, val_acc=0.924]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1760/1876 [00:10<00:00, 171.28it/s, loss=0.245, v_num=1, val_loss=0.261, val_acc=0.924]\n",
            "Epoch 1:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1800/1876 [00:10<00:00, 172.03it/s, loss=0.245, v_num=1, val_loss=0.261, val_acc=0.924]\n",
            "Epoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1840/1876 [00:10<00:00, 172.76it/s, loss=0.245, v_num=1, val_loss=0.261, val_acc=0.924]\n",
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1876/1876 [00:10<00:00, 173.34it/s, loss=0.205, v_num=1, val_loss=0.191, val_acc=0.943]\n",
            "Epoch 2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1720/1876 [00:10<00:00, 166.45it/s, loss=0.199, v_num=1, val_loss=0.191, val_acc=0.943]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1760/1876 [00:10<00:00, 167.09it/s, loss=0.199, v_num=1, val_loss=0.191, val_acc=0.943]\n",
            "Epoch 2:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1800/1876 [00:10<00:00, 167.94it/s, loss=0.199, v_num=1, val_loss=0.191, val_acc=0.943]\n",
            "Epoch 2:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1840/1876 [00:10<00:00, 168.62it/s, loss=0.199, v_num=1, val_loss=0.191, val_acc=0.943]\n",
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1876/1876 [00:11<00:00, 169.22it/s, loss=0.155, v_num=1, val_loss=0.156, val_acc=0.953]\n",
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1876/1876 [00:11<00:00, 169.12it/s, loss=0.155, v_num=1, val_loss=0.156, val_acc=0.953]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nht8AvMptY6I"
      },
      "source": [
        "### Testing\n",
        "\n",
        "To test a model, call `trainer.test(model)`.\n",
        "\n",
        "Or, if you've just trained a model, you can just call `trainer.test()` and Lightning will automatically test using the best saved checkpoint (conditioned on val_loss)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA151FkLtprO"
      },
      "source": [
        "trainer.test()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing:   6%|‚ñã         | 20/313 [00:00<00:01, 184.62it/s]/opt/miniconda3/envs/gridai/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:01<00:00, 211.11it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'val_acc': 0.9553999900817871, 'val_loss': 0.14908868074417114}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'val_loss': 0.14908868074417114, 'val_acc': 0.9553999900817871}]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3-3lbbNtr5T"
      },
      "source": [
        "### Bonus Tip\n",
        "\n",
        "You can keep calling `trainer.fit(model)` as many times as you'd like to continue training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFBwCbLet2r6"
      },
      "source": [
        "trainer.fit(model)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  | Name  | Type       | Params\n",
            "-------------------------------------\n",
            "0 | model | Sequential | 55.1 K\n",
            "-------------------------------------\n",
            "55.1 K    Trainable params\n",
            "0         Non-trainable params\n",
            "55.1 K    Total params\n",
            "0.220     Total estimated model params size (MB)\n",
            "Epoch 2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1720/1876 [00:10<13:15,  5.10s/it, loss=0.162, v_num=1, val_loss=0.0508, val_acc=0.984]   \n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1740/1876 [00:10<01:03,  2.13it/s, loss=0.162, v_num=1, val_loss=0.0508, val_acc=0.984]\n",
            "Epoch 2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1780/1876 [00:10<00:16,  5.90it/s, loss=0.162, v_num=1, val_loss=0.0508, val_acc=0.984]\n",
            "Epoch 2:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1820/1876 [00:10<00:05,  9.55it/s, loss=0.162, v_num=1, val_loss=0.0508, val_acc=0.984]\n",
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1876/1876 [00:10<00:00, 14.43it/s, loss=0.128, v_num=1, val_loss=0.117, val_acc=0.964] \n",
            "                                                              \u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TRyS5CCt3n9"
      },
      "source": [
        "\n",
        "# Start tensorboard.\n",
        "\n",
        "In Colab, you can use the TensorBoard magic function to view the logs that Lightning has created for you!\n",
        "```\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/\n",
        "```\n",
        "\n",
        "on VS Code \n",
        "```\n",
        "Python: Launch TensorBoard\n",
        "```"
      ]
    }
  ]
}