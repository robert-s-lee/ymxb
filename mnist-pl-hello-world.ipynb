{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "01-mnist-hello-world.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3710jvsc74a57bd016ccfe9cb28ae5249e710061f0e16cc760a03460c0b5a0514c93c438190c8559",
      "display_name": "Python 3.7.10 64-bit ('gridai': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robert-s-lee/ymxb/blob/master/mnist-pl-hello-world.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7XbLCXGkll9"
      },
      "source": [
        "# Introduction to Pytorch Lightning ⚡\n",
        "\n",
        "In this notebook, we'll go over the basics of lightning by preparing models to train on the [MNIST Handwritten Digits dataset](https://en.wikipedia.org/wiki/MNIST_database).\n",
        "\n",
        "---\n",
        "  - Give us a ⭐ [on Github](https://www.github.com/PytorchLightning/pytorch-lightning/)\n",
        "  - Check out [the documentation](https://pytorch-lightning.readthedocs.io/en/latest/)\n",
        "  - Join us [on Slack](https://join.slack.com/t/pytorch-lightning/shared_invite/zt-pw5v393p-qRaDgEk24~EjiZNBpSQFgQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LODD6w9ixlT"
      },
      "source": [
        "### Setup  \n",
        "Lightning is easy to install. Simply ```pip install pytorch-lightning```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK7-Gg69kMnG"
      },
      "source": [
        "! pip install pytorch-lightning --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4_TYnt_keJi"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics.functional import accuracy"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parser = ArgumentParser()\n",
        "parser.add_argument('--epochs', default=3, type=int)\n",
        "parser.add_argument('--gpu', default=0, type=int)\n",
        "parser.add_argument('--batch_size', default=32, type=int)\n",
        "parser.add_argument('--hidden_size', default=64, type=int)\n",
        "parser.add_argument('--lr', default=2e-4, type=float)\n",
        "parser.add_argument('--data', default=os.getcwd(), type=str)\n",
        "\n",
        "if __name__ == \"__main__.py\":    \n",
        "    args = parser.parse_args()      \n",
        "else:\n",
        "    args = parser.parse_args(\"\")    # take defaults in Jupyter "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHpyMPKFkVbZ"
      },
      "source": [
        "## Simplest example\n",
        "\n",
        "Here's the simplest most minimal example with just a training loop (no validation, no testing).\n",
        "\n",
        "**Keep in Mind** - A `LightningModule` *is* a PyTorch `nn.Module` - it just has a few more helpful features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7ELesz1kVQo"
      },
      "source": [
        "class MNISTModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        self.l1 = torch.nn.Linear(28 * 28, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        x, y = batch\n",
        "        loss = F.cross_entropy(self(x), y)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=args.lr)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIrtHg-Dv8TJ"
      },
      "source": [
        "By using the `Trainer` you automatically get:\n",
        "1. Tensorboard logging\n",
        "2. Model checkpointing\n",
        "3. Training and validation loop\n",
        "4. early-stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dk6Ykv8lI7X"
      },
      "source": [
        "# Init our model\n",
        "mnist_model = MNISTModel()\n",
        "\n",
        "# Init DataLoader from MNIST Dataset\n",
        "train_ds = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(train_ds, batch_size=args.batch_size)\n",
        "\n",
        "# Initialize a trainer\n",
        "trainer = pl.Trainer(gpus=args.gpu, max_epochs=args.epochs, progress_bar_refresh_rate=20)\n",
        "\n",
        "# Train the model ⚡\n",
        "trainer.fit(mnist_model, train_loader)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\n",
            "  | Name | Type   | Params\n",
            "--------------------------------\n",
            "0 | l1   | Linear | 7.9 K \n",
            "--------------------------------\n",
            "7.9 K     Trainable params\n",
            "0         Non-trainable params\n",
            "7.9 K     Total params\n",
            "0.031     Total estimated model params size (MB)\n",
            "Epoch 0:   2%|▏         | 40/1875 [00:00<00:07, 244.07it/s, loss=2.21, v_num=3]/opt/miniconda3/envs/gridai/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Epoch 2: 100%|██████████| 1875/1875 [00:06<00:00, 289.10it/s, loss=0.486, v_num=3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nht8AvMptY6I"
      },
      "source": [
        "### Testing\n",
        "\n",
        "To test a model, call `trainer.test(model)`.\n",
        "\n",
        "Or, if you've just trained a model, you can just call `trainer.test()` and Lightning will automatically test using the best saved checkpoint (conditioned on val_loss)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA151FkLtprO"
      },
      "source": [
        "trainer.test()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing:   6%|▋         | 20/313 [00:00<00:02, 134.56it/s]/opt/miniconda3/envs/gridai/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Testing: 100%|██████████| 313/313 [00:01<00:00, 207.14it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'val_acc': 0.965499997138977, 'val_loss': 0.11321506649255753}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'val_loss': 0.11321506649255753, 'val_acc': 0.965499997138977}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3-3lbbNtr5T"
      },
      "source": [
        "### Bonus Tip\n",
        "\n",
        "You can keep calling `trainer.fit(model)` as many times as you'd like to continue training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFBwCbLet2r6"
      },
      "source": [
        "trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TRyS5CCt3n9"
      },
      "source": [
        "\n",
        "# Start tensorboard.\n",
        "\n",
        "In Colab, you can use the TensorBoard magic function to view the logs that Lightning has created for you!\n",
        "```\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/\n",
        "```\n",
        "\n",
        "on VS Code \n",
        "```\n",
        "Python: Launch TensorBoard\n",
        "```"
      ]
    }
  ]
}